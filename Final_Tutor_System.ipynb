{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HIkzqeXsqGK8"
   },
   "outputs": [],
   "source": [
    "#Question Answering system using a pdf\\\n",
    "# Below is testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2X4pBfwqNWm",
    "outputId": "0c09f0d5-3293-40de-f177-e0ac6e423836"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: typer 0.12.5 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: typer 0.12.5 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Install the libraries\n",
    "!pip install -q langchain\n",
    "!pip install -q langchain_community\n",
    "!pip install -q langchain_openai\n",
    "!pip install -q openai\n",
    "!pip install -q langchain_core\n",
    "!pip install -q pypdf\n",
    "!pip install -q chromadb\n",
    "!pip install -q Flask\n",
    "!pip install -q markdown\n",
    "!pip install -q langchain guardrails-ai\n",
    "!pip install -q guardrails-ai\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enable anonymous metrics reporting? [Y/n]: ^C\n",
      "\u001b[31mAborted.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# configure with api key\n",
    "!guardrails configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mban_list...\u001b[0m\n",
      "\u001b[2K\u001b[32m[====]\u001b[0m Fetching manifestst\n",
      "\u001b[2K\u001b[32m[   =]\u001b[0m Downloading dependenciespendencies\n",
      "\u001b[1A\u001b[2K\u001b[?25l\u001b[32m[    ]\u001b[0m Running post-install setup\n",
      "\u001b[1A\u001b[2K✅Successfully installed guardrails/ban_list!\n",
      "\n",
      "\n",
      "\u001b[1mImport validator:\u001b[0m\n",
      "from guardrails.hub import BanList\n",
      "\n",
      "\u001b[1mGet more info:\u001b[0m\n",
      "\u001b[4;94mhttps://hub.guardrailsai.com/validator/guardrails/ban_list\u001b[0m\n",
      "\n",
      "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mbias_check...\u001b[0m\n",
      "\u001b[2K\u001b[32m[   =]\u001b[0m Fetching manifestst\n",
      "\u001b[2K\u001b[32m[====]\u001b[0m Downloading dependenciespendencies\n",
      "\u001b[2K\u001b[32m[==  ]\u001b[0m Running post-install setuptall setup2025-01-11 13:22:09.148117: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2K\u001b[32m[=== ]\u001b[0m Running post-install setupAll model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at d4data/bias-detection-model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n",
      "\u001b[2K\u001b[32m[    ]\u001b[0m Running post-install setup\n",
      "\u001b[1A\u001b[2K2025-01-11 13:22:51.680872: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "✅Successfully installed guardrails/bias_check!\n",
      "\n",
      "\n",
      "\u001b[1mImport validator:\u001b[0m\n",
      "from guardrails.hub import BiasCheck\n",
      "\n",
      "\u001b[1mGet more info:\u001b[0m\n",
      "\u001b[4;94mhttps://hub.guardrailsai.com/validator/guardrails/bias_check\u001b[0m\n",
      "\n",
      "2025-01-11 13:23:03.133801: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mnsfw_text...\u001b[0m\n",
      "\u001b[2K\u001b[32m[    ]\u001b[0m Fetching manifestst\n",
      "\u001b[2K\u001b[32m[=   ]\u001b[0m Downloading dependenciespendencies\n",
      "\u001b[2K\u001b[32m[====]\u001b[0m Running post-install setup2025-01-11 13:23:17.974535: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2K\u001b[32m[   =]\u001b[0m Running post-install setup[nltk_data] Error loading punkt_tab: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "\u001b[2K\u001b[32m[    ]\u001b[0m Running post-install setupDevice set to use mps:0\n",
      "\u001b[2K\u001b[32m[ ===]\u001b[0m Running post-install setup\n",
      "\u001b[1A\u001b[2K✅Successfully installed guardrails/nsfw_text!\n",
      "\n",
      "\n",
      "\u001b[1mImport validator:\u001b[0m\n",
      "from guardrails.hub import NSFWText\n",
      "\n",
      "\u001b[1mGet more info:\u001b[0m\n",
      "\u001b[4;94mhttps://hub.guardrailsai.com/validator/guardrails/nsfw_text\u001b[0m\n",
      "\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "2025-01-11 13:23:32.457092: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mprofanity_free...\u001b[0m\n",
      "\u001b[2K\u001b[32m[ ===]\u001b[0m Fetching manifestst\n",
      "\u001b[2K\u001b[32m[=   ]\u001b[0m Downloading dependenciespendencies\n",
      "\u001b[2K\u001b[32m[    ]\u001b[0m Running post-install setuptall setup\n",
      "\u001b[1A\u001b[2K✅Successfully installed guardrails/profanity_free!\n",
      "\n",
      "\n",
      "\u001b[1mImport validator:\u001b[0m\n",
      "from guardrails.hub import ProfanityFree\n",
      "\n",
      "\u001b[1mGet more info:\u001b[0m\n",
      "\u001b[4;94mhttps://hub.guardrailsai.com/validator/guardrails/profanity_free\u001b[0m\n",
      "\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "2025-01-11 13:24:43.239228: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mlogic_check...\u001b[0m\n",
      "\u001b[2K\u001b[32m[   =]\u001b[0m Fetching manifestst\n",
      "\u001b[2K\u001b[32m[  ==]\u001b[0m Downloading dependenciespendencies\n",
      "\u001b[1A\u001b[2K\u001b[?25l\u001b[32m[    ]\u001b[0m Running post-install setup\n",
      "\u001b[1A\u001b[2K✅Successfully installed guardrails/logic_check!\n",
      "\n",
      "\n",
      "\u001b[1mImport validator:\u001b[0m\n",
      "from guardrails.hub import LogicCheck\n",
      "\n",
      "\u001b[1mGet more info:\u001b[0m\n",
      "\u001b[4;94mhttps://hub.guardrailsai.com/validator/guardrails/logic_check\u001b[0m\n",
      "\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "2025-01-11 13:25:00.893306: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/cartesia/\u001b[0m\u001b[95mmentions_drugs...\u001b[0m\n",
      "\u001b[2K\u001b[32m[  ==]\u001b[0m Fetching manifestst\n",
      "\u001b[2K\u001b[32m[==  ]\u001b[0m Downloading dependenciespendencies\n",
      "\u001b[1A\u001b[2K\u001b[?25l\u001b[32m[    ]\u001b[0m Running post-install setup\n",
      "\u001b[1A\u001b[2K✅Successfully installed cartesia/mentions_drugs!\n",
      "\n",
      "\n",
      "\u001b[1mImport validator:\u001b[0m\n",
      "from guardrails.hub import MentionsDrugs\n",
      "\n",
      "\u001b[1mGet more info:\u001b[0m\n",
      "\u001b[4;94mhttps://hub.guardrailsai.com/validator/cartesia/mentions_drugs\u001b[0m\n",
      "\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "2025-01-11 13:25:18.350342: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mpoliteness_check...\u001b[0m\n",
      "\u001b[2K\u001b[32m[ ===]\u001b[0m Fetching manifestst\n",
      "\u001b[2K\u001b[32m[=== ]\u001b[0m Downloading dependenciespendencies\n",
      "\u001b[2K\u001b[32m[    ]\u001b[0m Running post-install setuptall setup[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "\u001b[2K\u001b[32m[==  ]\u001b[0m Running post-install setup2025-01-11 13:25:38.322802: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2K\u001b[32m[=   ]\u001b[0m Running post-install setup/Users/sridhars/Library/Python/3.12/lib/python/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "\u001b[2K\u001b[32m[ ===]\u001b[0m Running post-install setup\n",
      "\u001b[1A\u001b[2K/Users/sridhars/Library/Python/3.12/lib/python/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "✅Successfully installed guardrails/politeness_check!\n",
      "\n",
      "\n",
      "\u001b[1mImport validator:\u001b[0m\n",
      "from guardrails.hub import PolitenessCheck\n",
      "\n",
      "\u001b[1mGet more info:\u001b[0m\n",
      "\u001b[4;94mhttps://hub.guardrailsai.com/validator/guardrails/politeness_check\u001b[0m\n",
      "\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "2025-01-11 13:26:02.075849: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/sridhars/Library/Python/3.12/lib/python/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mtoxic_language...\u001b[0m\n",
      "\u001b[2K\u001b[32m[    ]\u001b[0m Fetching manifestst\n",
      "\u001b[2K\u001b[32m[   =]\u001b[0m Downloading dependenciespendencies\n",
      "\u001b[2K\u001b[32m[    ]\u001b[0m Running post-install setuptall setup[nltk_data] Error loading punkt_tab: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "Downloading: \"https://github.com/unitaryai/detoxify/releases/download/v0.1.2/unbiased-albert-c8519128.ckpt\" to /Users/sridhars/.cache/torch/hub/checkpoints/unbiased-albert-c8519128.ckpt\n",
      "\u001b[2K\u001b[32m[  ==]\u001b[0m Running post-install setupTraceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py\", line 1344, in do_open\n",
      "    h.request(req.get_method(), req.selector, req.data, headers,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py\", line 1336, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py\", line 1382, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py\", line 1331, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py\", line 1091, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py\", line 1035, in send\n",
      "    self.connect()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py\", line 1477, in connect\n",
      "    self.sock = self._context.wrap_socket(self.sock,\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py\", line 455, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py\", line 1042, in _create\n",
      "    self.do_handshake()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py\", line 1320, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sridhars/Library/Python/3.12/lib/python/site-packages/guardrails_grhub_toxic_language/post-install.py\", line 54, in <module>\n",
      "    model = detoxify.Detoxify(\"unbiased-small\")\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sridhars/Library/Python/3.12/lib/python/site-packages/detoxify/detoxify.py\", line 104, in __init__\n",
      "    self.model, self.tokenizer, self.class_names = load_checkpoint(\n",
      "                                                   ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sridhars/Library/Python/3.12/lib/python/site-packages/detoxify/detoxify.py\", line 41, in load_checkpoint\n",
      "    loaded = torch.hub.load_state_dict_from_url(checkpoint_path, map_location=device)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sridhars/Library/Python/3.12/lib/python/site-packages/torch/hub.py\", line 766, in load_state_dict_from_url\n",
      "    download_url_to_file(url, cached_file, hash_prefix, progress=progress)\n",
      "  File \"/Users/sridhars/Library/Python/3.12/lib/python/site-packages/torch/hub.py\", line 620, in download_url_to_file\n",
      "    u = urlopen(req)\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py\", line 215, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py\", line 515, in open\n",
      "    response = self._open(req, data)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py\", line 532, in _open\n",
      "    result = self._call_chain(self.handle_open, protocol, protocol +\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py\", line 492, in _call_chain\n",
      "    result = func(*args)\n",
      "             ^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py\", line 1392, in https_open\n",
      "    return self.do_open(http.client.HTTPSConnection, req,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py\", line 1347, in do_open\n",
      "    raise URLError(err)\n",
      "urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)>\n",
      "\u001b[2K\u001b[32m[=   ]\u001b[0m Running post-install setupERROR:guardrails-cli:Failed to run post install script for guardrails/toxic_language\n",
      "Exit code: 1\n",
      "stdout: b''\n",
      "\u001b[2K\u001b[32m[==  ]\u001b[0m Running post-install setup\n",
      "\u001b[1A\u001b[2KERROR:guardrails-cli:Failed to run post install script for guardrails/toxic_language\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# install guardrails libraries\n",
    "!guardrails hub install hub://guardrails/ban_list\n",
    "!guardrails hub install hub://guardrails/bias_check\n",
    "!guardrails hub install hub://guardrails/nsfw_text\n",
    "!guardrails hub install hub://guardrails/profanity_free\n",
    "!guardrails hub install hub://guardrails/logic_check\n",
    "!guardrails hub install hub://cartesia/mentions_drugs\n",
    "!guardrails hub install hub://guardrails/politeness_check\n",
    "!guardrails hub install hub://guardrails/toxic_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Mtv-MxsyqQAn"
   },
   "outputs": [],
   "source": [
    "##Imports for Baseline QA Pipeline\n",
    "from langchain.document_loaders import PyPDFLoader # for loading the pdf\n",
    "from langchain_openai import OpenAIEmbeddings # for creating embeddings\n",
    "from langchain.vectorstores import Chroma # for the vectorization part\n",
    "from langchain.chains import RetrievalQA #For the retrieval QA chain part # apparently deprecated\n",
    "from langchain_openai import ChatOpenAI #for getting an LLM for QA chain\n",
    "#from langchain_core.output_parsers import StrOutputParser #Not used currently, leaving, as can be used for parsing output from LLM\n",
    "#from langchain_core.runnables import RunnablePassthrough #Not used currently, leaving, as can be used for getting LLM output\n",
    "from langchain.prompts import ChatPromptTemplate #for setting up prompts\n",
    "\n",
    "\n",
    "#Guardrails stuff\n",
    "from guardrails import Guard, OnFailAction\n",
    "from guardrails.hub import BanList, BiasCheck, NSFWText, ProfanityFree, LogicCheck, MentionsDrugs, PolitenessCheck #, ToxicLanguage# Updated import\n",
    "#import guardrails.hub #which needs fuzzysearch, which is already downloaded\n",
    "#from guardrails.datatypes import String\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QOSdAp8mqRX5"
   },
   "outputs": [],
   "source": [
    "#Setup openai key\n",
    "import os\n",
    "import openai\n",
    "from getpass import getpass\n",
    "OPENAI_API_KEY = getpass()\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "a96_ARBwtZEE",
    "outputId": "8d8525b4-b27a-4791-dff9-a1fe61819179"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 10.8M  100 10.8M    0     0  11.9M      0 --:--:-- --:--:-- --:--:-- 11.9M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    69    0    69    0     0    164      0 --:--:-- --:--:-- --:--:--   164\n"
     ]
    }
   ],
   "source": [
    "#Download a sample pdf\n",
    "!curl https://www.mrbigler.com/downloads/Notes-Physics-1.pdf >Notes_Physics.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from guardrails import Guard\n",
    "\n",
    "\n",
    "guard = Guard().use_many(\n",
    "    BiasCheck(\n",
    "        threshold=0.5,\n",
    "        on_fail=\"exception\"\n",
    "    ),\n",
    "\n",
    "    NSFWText(\n",
    "        threshold=0.8,\n",
    "        validation_method=\"sentence\"\n",
    "    ),\n",
    "\n",
    "    ProfanityFree(\n",
    "        on_fail = \"exception\"\n",
    "    ),\n",
    "\n",
    "    LogicCheck(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        on_fail=\"exception\"\n",
    "    ),\n",
    "\n",
    "    MentionsDrugs(\n",
    "        on_fail = \"exception\"\n",
    "    ),\n",
    "\n",
    "    PolitenessCheck(\n",
    "        llm_callable=\"gpt-3.5-turbo\",\n",
    "        on_fail = \"exception\"\n",
    "    ),\n",
    "\n",
    "    ToxicLanguage(\n",
    "        threshold=0.5,\n",
    "        validation_method=\"sentence\",\n",
    "        on_fail=\"exception\"\n",
    "    )\n",
    "\n",
    ")\n",
    "\n",
    "try:\n",
    "  guard.validate(\"I hate you!\")\n",
    "  print(\"hello\")\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6reJ1IoqVrK"
   },
   "outputs": [],
   "source": [
    "#Setup Base QA system pipeline\n",
    "class BaseQAPipeline:\n",
    "    def __init__(self):\n",
    "        self.doc = \"tutor_textbook.pdf\"\n",
    "        self.loader = PyPDFLoader(self.doc)\n",
    "\n",
    "        # Load the document and store it in the 'data' variable\n",
    "        self.data = self.loader.load_and_split()\n",
    "\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.vectordb = Chroma.from_documents(self.data, embedding=self.embeddings,\n",
    "                                 persist_directory=\".\")\n",
    "\n",
    "        # Initialize a language model with ChatOpenAI\n",
    "        self.llm = ChatOpenAI(model_name= 'gpt-3.5-turbo', temperature=0.6)\n",
    "\n",
    "        #Setup a prompt template\n",
    "        template = \"\"\"\\\n",
    "        You are an assistant for question-answering tasks.\n",
    "\n",
    "        Use the following pieces of retrieved context to answer the question.\n",
    "\n",
    "        If either the PDF or the question are not related to each other or not \n",
    "        related to any educational standard, state the following: This content is \n",
    "        not related to any educational purposes. \n",
    "\n",
    "        For example, if topics are not the same, like a java textbook is given, \n",
    "        however, the user asks about a physics question, state the following: This\n",
    "        content is not related to the inputted textbook, please select another textbook\n",
    "        and try again.\n",
    "\n",
    "        If you don't know the answer, just say that you don't know.\n",
    "\n",
    "        Use three sentences maximum and keep the answer concise. \n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Answer:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "        chain_type_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "\n",
    "\n",
    "        # 1. Vectorstore-based retriever\n",
    "        self.vectorstore_retriever = self.vectordb.as_retriever()\n",
    "\n",
    "        # Initialize a RetrievalQA chain with the language model and vector database retriever\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(self.llm, retriever= self.vectorstore_retriever, chain_type_kwargs=chain_type_kwargs)\n",
    "        self.chat_history = []  # Initialize chat history\n",
    "\n",
    "    def update_chat_history(self, question, answer):\n",
    "        self.chat_history.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "    def build_combined_context(self):\n",
    "        \"\"\"Combine chat history and document context.\"\"\"\n",
    "        # Combine all previous chat history\n",
    "        chat_context = \"\\n\".join([f\"Q: {entry['question']}\\nA: {entry['answer']}\" for entry in self.chat_history])\n",
    "        \n",
    "        # Fetch relevant context from the vector store based on the current question\n",
    "        if self.chat_history:\n",
    "            current_question = self.chat_history[-1]['question']\n",
    "            context_from_db = self.vectorstore_retriever.get_relevant_documents(current_question)\n",
    "        else:\n",
    "            context_from_db = self.vectorstore_retriever.get_relevant_documents(\"\")\n",
    "\n",
    "        # Convert the list of context documents into a string\n",
    "        context_str = \"\\n\".join([doc.page_content for doc in context_from_db])\n",
    "\n",
    "        # Combine both chat history and the document context\n",
    "        combined_context = f\"Chat history:\\n{chat_context}\\n\\nContext from the document:\\n{context_str}\"\n",
    "        \n",
    "        return combined_context\n",
    "\n",
    "\n",
    "    def invoke(self, input_dict):\n",
    "        question = input_dict.get(\"question\")\n",
    "        #here put a if-else that returns true or false depending on if question passes\n",
    "        # guardrails checks\n",
    "        if (self.guardrails(question) == False):\n",
    "          print(\"It has failed (this is only a message to debug)\\n\")\n",
    "          return \"Sorry, please ask another question\"\n",
    "        \n",
    "        combined_context = self.build_combined_context()\n",
    "\n",
    "        result = self.qa_chain.invoke({\n",
    "            \"query\": question,\n",
    "            \"context\": combined_context\n",
    "        })\n",
    "\n",
    "        self.update_chat_history(question, result['result'])\n",
    "        if (self.guardrails(question) == False):\n",
    "          print(\"It has failed (this is only a message to debug)\\n\")\n",
    "          return \"Sorry, please ask another question\"\n",
    "        else:\n",
    "          return result\n",
    "\n",
    "    def guardrails(self, input):\n",
    "      #if guardrails return true send back whatever the input is,\n",
    "      #else send back an error message\n",
    "      try:\n",
    "        guard.validate(input)\n",
    "        return True\n",
    "      except Exception as e:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup GenerateStudyPlan pipeline\n",
    "class GenerateStudyPlan:\n",
    "    def __init__(self):\n",
    "        self.doc = \"tutor_textbook.pdf\"\n",
    "        self.loader = PyPDFLoader(self.doc)\n",
    "\n",
    "        # Load the document and store it in the 'data' variable\n",
    "        self.data = self.loader.load_and_split()\n",
    "\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.vectordb = Chroma.from_documents(self.data, embedding=self.embeddings,\n",
    "                                 persist_directory=\".\")\n",
    "\n",
    "        # Initialize a language model with ChatOpenAI\n",
    "        self.llm = ChatOpenAI(model_name= 'gpt-3.5-turbo', temperature=0.6)\n",
    "\n",
    "        #Setup a prompt template\n",
    "        template = \"\"\"\\\n",
    "            You are an assistant for generating study plans on a singular subject.\n",
    "\n",
    "        Use the following pieces of retrieved context to answer the question.\n",
    "\n",
    "        If the user has given a topic to study or topics that they need focus on,\n",
    "        make the plan more focused on those topics.\n",
    "\n",
    "        Give a nice and detailed study plan. If the user doesnt specify a type of \n",
    "        study plan (schedule wise), you can come up with your own, hourly or daily \n",
    "        plan, or you can ask the user to give the same instructions but with the \n",
    "        study plan of their choice. \n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Answer:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "        chain_type_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "\n",
    "        # 1. Vectorstore-based retriever\n",
    "        self.vectorstore_retriever = self.vectordb.as_retriever()\n",
    "\n",
    "        # Initialize a RetrievalQA chain with the language model and vector database retriever\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(self.llm, retriever= self.vectorstore_retriever, chain_type_kwargs=chain_type_kwargs)\n",
    "        self.chat_history = []  # Initialize chat history\n",
    "\n",
    "    def update_chat_history(self, question, answer):\n",
    "        self.chat_history.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "    def build_combined_context(self):\n",
    "        \"\"\"Combine chat history and document context.\"\"\"\n",
    "        # Combine all previous chat history\n",
    "        chat_context = \"\\n\".join([f\"Q: {entry['question']}\\nA: {entry['answer']}\" for entry in self.chat_history])\n",
    "        \n",
    "        # Fetch relevant context from the vector store based on the current question\n",
    "        if self.chat_history:\n",
    "            current_question = self.chat_history[-1]['question']\n",
    "            context_from_db = self.vectorstore_retriever.get_relevant_documents(current_question)\n",
    "        else:\n",
    "            context_from_db = self.vectorstore_retriever.get_relevant_documents(\"\")\n",
    "\n",
    "        # Convert the list of context documents into a string\n",
    "        context_str = \"\\n\".join([doc.page_content for doc in context_from_db])\n",
    "\n",
    "        # Combine both chat history and the document context\n",
    "        combined_context = f\"Chat history:\\n{chat_context}\\n\\nContext from the document:\\n{context_str}\"\n",
    "        \n",
    "        return combined_context\n",
    "\n",
    "\n",
    "    def invoke(self, input_dict):\n",
    "        question = input_dict.get(\"question\")\n",
    "        #here put a if-else that returns true or false depending on if question passes\n",
    "        # guardrails checks\n",
    "        if (self.guardrails(question) == False):\n",
    "          print(\"It has failed (this is only a message to debug)\\n\")\n",
    "          return \"Sorry, please ask another question\"\n",
    "        \n",
    "        combined_context = self.build_combined_context()\n",
    "\n",
    "        result = self.qa_chain.invoke({\n",
    "            \"query\": question,\n",
    "            \"context\": combined_context\n",
    "        })\n",
    "\n",
    "        self.update_chat_history(question, result['result'])\n",
    "        if (self.guardrails(question) == False):\n",
    "          print(\"It has failed (this is only a message to debug)\\n\")\n",
    "          return \"Sorry, please ask another question\"\n",
    "        else:\n",
    "          return result\n",
    "\n",
    "    def guardrails(self, input):\n",
    "        #if guardrails return true send back whatever the input is,\n",
    "        #else send back an error message\n",
    "        try:\n",
    "            guard.validate(input)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "0tUCW5XKgxul",
    "outputId": "612cbf9b-70cf-415d-f556-2467c118ba7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is running...\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:8080\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, redirect, url_for\n",
    "import markdown\n",
    "\n",
    "filepath = \"./tutor_textbook.pdf\"\n",
    "ALLOWED_EXTENSIONS = {'txt', 'pdf', 'docx', 'png', 'jpg', 'jpeg', 'gif'}\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\",methods=[\"GET\"])\n",
    "def index():\n",
    "   return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/tutor-ai\", methods=[\"GET\", \"POST\"])\n",
    "def tutor_ai():\n",
    "    global url_data, prompt_data  # Access global variables\n",
    "\n",
    "    if request.method == \"POST\":\n",
    "        url_data = request.form.get(\"url\")\n",
    "        print(\"URL: \", url_data)\n",
    "        if 'file' not in request.files:\n",
    "            print('No file uploaded!')\n",
    "        else:\n",
    "          file = request.files['file']\n",
    "          file.save(filepath)\n",
    "          print(\"File saved:\", filepath)\n",
    "        if (url_data != \"\"):\n",
    "            subprocess.check_call(\"curl\", url_data, \">\", \"tutor_textbook.pdf\")\n",
    "        print(\"File: \",file)\n",
    "        prompt_data = request.form.get(\"prompt\")\n",
    "        base_qa_pipeline = BaseQAPipeline()\n",
    "        result = base_qa_pipeline.invoke({'question' : prompt_data})\n",
    "        print(result)\n",
    "        return render_template(\"tutor-ai.html\", result=result)\n",
    "\n",
    "    return render_template(\"tutor-ai.html\")\n",
    "\n",
    "@app.route('/how-it-works', methods=['GET'])\n",
    "def how_it_works():\n",
    "    return render_template('how-it-works.html')\n",
    "\n",
    "@app.route('/generate-plan', methods=['GET', \"POST\"])\n",
    "def generate_plan():\n",
    "    if request.method == \"POST\":\n",
    "        if 'file' not in request.files:\n",
    "            print('No file uploaded!')\n",
    "        else:\n",
    "          file = request.files['file']\n",
    "          file.save(filepath)\n",
    "          print(\"File saved:\", filepath)\n",
    "        print(\"File: \",file)\n",
    "        prompt_data = request.form.get(\"prompt\")\n",
    "        generate_plan = GenerateStudyPlan()\n",
    "        result = generate_plan.invoke({'question' : prompt_data})\n",
    "        result['result'] = markdown.markdown(result['result'])\n",
    "        print(result)\n",
    "        return render_template(\"generate-plan.html\", result=result)\n",
    "\n",
    "    return render_template(\"generate-plan.html\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #from waitress import serve\n",
    "    #serve(app, host=\"0.0.0.0\", port=8081)\n",
    "    # above code is for SERVER\n",
    "    #below code right now is to debug\n",
    "    print(\"Server is running...\")\n",
    "    app.run(port=8080)\n",
    "    print(\"Stopping Server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "R3h2ufh5BSFD",
    "outputId": "a2af90e6-1d62-48fe-e820-7ed384effc2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/2.5 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m605.5/605.5 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
      "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting Flask==2.3.2\n",
      "  Downloading Flask-2.3.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: Werkzeug>=2.3.3 in /usr/local/lib/python3.10/dist-packages (from Flask==2.3.2) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask==2.3.2) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask==2.3.2) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from Flask==2.3.2) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from Flask==2.3.2) (1.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->Flask==2.3.2) (3.0.2)\n",
      "Downloading Flask-2.3.2-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Flask\n",
      "  Attempting uninstall: Flask\n",
      "    Found existing installation: Flask 3.1.0\n",
      "    Uninstalling Flask-3.1.0:\n",
      "      Successfully uninstalled Flask-3.1.0\n",
      "Successfully installed Flask-2.3.2\n",
      "Using pip 24.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
      "Collecting openai==1.57.0\n",
      "  Obtaining dependency information for openai==1.57.0 from https://files.pythonhosted.org/packages/ab/2d/eb8539a2d5809eb78508633a8faa8df7745960e99af0388310c43b2c0be1/openai-1.57.0-py3-none-any.whl.metadata\n",
      "  Downloading openai-1.57.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai==1.57.0)\n",
      "  Obtaining dependency information for anyio<5,>=3.5.0 from https://files.pythonhosted.org/packages/a0/7a/4daaf3b6c08ad7ceffea4634ec206faeff697526421c20f07628c7372156/anyio-4.7.0-py3-none-any.whl.metadata\n",
      "  Downloading anyio-4.7.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai==1.57.0)\n",
      "  Obtaining dependency information for distro<2,>=1.7.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai==1.57.0)\n",
      "  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai==1.57.0)\n",
      "  Obtaining dependency information for jiter<1,>=0.4.0 from https://files.pythonhosted.org/packages/4d/a0/3993cda2e267fe679b45d0bcc2cef0b4504b0aa810659cdae9737d6bace9/jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.57.0)\n",
      "  Obtaining dependency information for pydantic<3,>=1.9.0 from https://files.pythonhosted.org/packages/f3/26/3e1bbe954fde7ee22a6e7d31582c642aad9e84ffe4b5fb61e63b87cd326f/pydantic-2.10.4-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting sniffio (from openai==1.57.0)\n",
      "  Obtaining dependency information for sniffio from https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl.metadata\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai==1.57.0)\n",
      "  Obtaining dependency information for tqdm>4 from https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions<5,>=4.11 (from openai==1.57.0)\n",
      "  Obtaining dependency information for typing-extensions<5,>=4.11 from https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.5.0->openai==1.57.0)\n",
      "  Obtaining dependency information for exceptiongroup>=1.0.2 from https://files.pythonhosted.org/packages/02/cc/b7e31358aac6ed1ef2bb790a9746ac2c69bcb3c8588b41616914eb106eaf/exceptiongroup-1.2.2-py3-none-any.whl.metadata\n",
      "  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai==1.57.0)\n",
      "  Obtaining dependency information for idna>=2.8 from https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl.metadata\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai==1.57.0)\n",
      "  Obtaining dependency information for certifi from https://files.pythonhosted.org/packages/a5/32/8f6669fc4798494966bf446c8c4a162e0b5d893dff088afddf76414f70e1/certifi-2024.12.14-py3-none-any.whl.metadata\n",
      "  Downloading certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.57.0)\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/87/f5/72347bc88306acb359581ac4d52f23c0ef445b57157adedb9aee0cd689d2/httpcore-1.0.7-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.57.0)\n",
      "  Obtaining dependency information for h11<0.15,>=0.13 from https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai==1.57.0)\n",
      "  Obtaining dependency information for annotated-types>=0.6.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->openai==1.57.0)\n",
      "  Obtaining dependency information for pydantic-core==2.27.2 from https://files.pythonhosted.org/packages/32/90/3b15e31b88ca39e9e626630b4c4a1f5a0dfd09076366f4219429e6786076/pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Downloading openai-1.57.0-py3-none-any.whl (389 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.9/389.9 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.7.0-py3-none-any.whl (93 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.8/431.8 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.9/164.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: typing-extensions, tqdm, sniffio, jiter, idna, h11, exceptiongroup, distro, certifi, annotated-types, pydantic-core, httpcore, anyio, pydantic, httpx, openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.12.2.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Removing file or directory /usr/local/bin/tqdm\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/tqdm-4.67.1.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/tqdm/\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  changing mode of /usr/local/bin/tqdm to 755\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.3.1\n",
      "    Uninstalling sniffio-1.3.1:\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/sniffio-1.3.1.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/sniffio/\n",
      "      Successfully uninstalled sniffio-1.3.1\n",
      "  Attempting uninstall: jiter\n",
      "    Found existing installation: jiter 0.8.2\n",
      "    Uninstalling jiter-0.8.2:\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/jiter-0.8.2.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/jiter/\n",
      "      Successfully uninstalled jiter-0.8.2\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/idna-3.10.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/idna/\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/h11-0.14.0.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/h11/\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: exceptiongroup\n",
      "    Found existing installation: exceptiongroup 1.2.2\n",
      "    Uninstalling exceptiongroup-1.2.2:\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/exceptiongroup-1.2.2.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/exceptiongroup/\n",
      "      Successfully uninstalled exceptiongroup-1.2.2\n",
      "  Attempting uninstall: distro\n",
      "    Found existing installation: distro 1.9.0\n",
      "    Uninstalling distro-1.9.0:\n",
      "      Removing file or directory /usr/local/bin/distro\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/distro-1.9.0.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/distro/\n",
      "      Successfully uninstalled distro-1.9.0\n",
      "  changing mode of /usr/local/bin/distro to 755\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.12.14\n",
      "    Uninstalling certifi-2024.12.14:\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/certifi-2024.12.14.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/certifi/\n",
      "      Successfully uninstalled certifi-2024.12.14\n",
      "  Attempting uninstall: annotated-types\n",
      "    Found existing installation: annotated-types 0.7.0\n",
      "    Uninstalling annotated-types-0.7.0:\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/annotated_types-0.7.0.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/annotated_types/\n",
      "      Successfully uninstalled annotated-types-0.7.0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.27.1\n",
      "    Uninstalling pydantic_core-2.27.1:\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/pydantic_core-2.27.1.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/pydantic_core/\n",
      "      Successfully uninstalled pydantic_core-2.27.1\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.7\n",
      "    Uninstalling httpcore-1.0.7:\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/httpcore-1.0.7.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/httpcore/\n",
      "      Successfully uninstalled httpcore-1.0.7\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 3.7.1\n",
      "    Uninstalling anyio-3.7.1:\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio-3.7.1.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/__init__.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/_backends/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/_core/__init__.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/_core/_compat.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/_core/_eventloop.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/_core/_exceptions.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/_core/_fileio.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/_core/_resources.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/_core/_signals.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/_core/_sockets.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/_core/_streams.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/_core/_subprocesses.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/_core/_synchronization.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/_core/_tasks.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/_core/_testing.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/_core/_typedattr.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/abc/__init__.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/abc/_resources.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/abc/_sockets.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/abc/_streams.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/abc/_subprocesses.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/abc/_tasks.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/abc/_testing.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/from_thread.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/lowlevel.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/py.typed\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/pytest_plugin.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/streams/__init__.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/streams/buffered.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/streams/file.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/streams/memory.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/streams/stapled.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/streams/text.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/streams/tls.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/to_process.py\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\n",
      "      Successfully uninstalled anyio-3.7.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.10.3\n",
      "    Uninstalling pydantic-2.10.3:\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/pydantic-2.10.3.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/pydantic/\n",
      "      Successfully uninstalled pydantic-2.10.3\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Removing file or directory /usr/local/bin/httpx\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/httpx-0.28.1.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/httpx/\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "  changing mode of /usr/local/bin/httpx to 755\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.59.3\n",
      "    Uninstalling openai-1.59.3:\n",
      "      Removing file or directory /usr/local/bin/openai\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/openai-1.59.3.dist-info/\n",
      "      Removing file or directory /usr/local/lib/python3.10/dist-packages/openai/\n",
      "      Successfully uninstalled openai-1.59.3\n",
      "  changing mode of /usr/local/bin/openai to 755\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.7.0 which is incompatible.\n",
      "langchain-openai 0.2.14 requires openai<2.0.0,>=1.58.1, but you have openai 1.57.0 which is incompatible.\n",
      "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.7.0 anyio-4.7.0 certifi-2024.12.14 distro-1.9.0 exceptiongroup-1.2.2 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 jiter-0.8.2 openai-1.57.0 pydantic-2.10.4 pydantic-core-2.27.2 sniffio-1.3.1 tqdm-4.67.1 typing-extensions-4.12.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "323e97e1bc764d5d9d367e0ac7212405",
       "pip_warning": {
        "packages": [
         "certifi"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx) (4.7.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx) (4.12.2)\n",
      "Please enter Open AI KEY\n",
      "··········\n",
      "https://qg8uz4v9w7d-496ff2e9c6d22116-5000-colab.googleusercontent.com/\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "ERROR:__main__:Exception on / [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2190, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1486, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1484, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1469, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "  File \"<ipython-input-1-4ab8419cdd29>\", line 119, in index\n",
      "    return render_template(\"index.html\")\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/templating.py\", line 150, in render_template\n",
      "    template = app.jinja_env.get_or_select_template(template_name_or_list)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/jinja2/environment.py\", line 1084, in get_or_select_template\n",
      "    return self.get_template(template_name_or_list, parent, globals)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/jinja2/environment.py\", line 1013, in get_template\n",
      "    return self._load_template(name, globals)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/jinja2/environment.py\", line 972, in _load_template\n",
      "    template = self.loader.load(self, name, self.make_globals(globals))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/jinja2/loaders.py\", line 126, in load\n",
      "    source, filename, uptodate = self.get_source(environment, name)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/templating.py\", line 64, in get_source\n",
      "    return self._get_source_fast(environment, template)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/templating.py\", line 98, in _get_source_fast\n",
      "    raise TemplateNotFound(template)\n",
      "jinja2.exceptions.TemplateNotFound: index.html\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Jan/2025 20:16:35] \"\u001b[35m\u001b[1mGET /?authuser=0 HTTP/1.1\u001b[0m\" 500 -\n",
      "ERROR:__main__:Exception on / [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2190, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1486, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1484, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1469, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "  File \"<ipython-input-1-4ab8419cdd29>\", line 119, in index\n",
      "    return render_template(\"index.html\")\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/templating.py\", line 150, in render_template\n",
      "    template = app.jinja_env.get_or_select_template(template_name_or_list)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/jinja2/environment.py\", line 1084, in get_or_select_template\n",
      "    return self.get_template(template_name_or_list, parent, globals)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/jinja2/environment.py\", line 1013, in get_template\n",
      "    return self._load_template(name, globals)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/jinja2/environment.py\", line 972, in _load_template\n",
      "    template = self.loader.load(self, name, self.make_globals(globals))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/jinja2/loaders.py\", line 126, in load\n",
      "    source, filename, uptodate = self.get_source(environment, name)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/templating.py\", line 64, in get_source\n",
      "    return self._get_source_fast(environment, template)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/templating.py\", line 98, in _get_source_fast\n",
      "    raise TemplateNotFound(template)\n",
      "jinja2.exceptions.TemplateNotFound: index.html\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Jan/2025 20:16:36] \"\u001b[35m\u001b[1mGET /?authuser=0 HTTP/1.1\u001b[0m\" 500 -\n",
      "ERROR:__main__:Exception on / [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2190, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1486, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1484, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1469, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "  File \"<ipython-input-1-4ab8419cdd29>\", line 119, in index\n",
      "    return render_template(\"index.html\")\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/templating.py\", line 150, in render_template\n",
      "    template = app.jinja_env.get_or_select_template(template_name_or_list)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/jinja2/environment.py\", line 1084, in get_or_select_template\n",
      "    return self.get_template(template_name_or_list, parent, globals)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/jinja2/environment.py\", line 1013, in get_template\n",
      "    return self._load_template(name, globals)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/jinja2/environment.py\", line 972, in _load_template\n",
      "    template = self.loader.load(self, name, self.make_globals(globals))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/jinja2/loaders.py\", line 126, in load\n",
      "    source, filename, uptodate = self.get_source(environment, name)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/templating.py\", line 64, in get_source\n",
      "    return self._get_source_fast(environment, template)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/flask/templating.py\", line 98, in _get_source_fast\n",
      "    raise TemplateNotFound(template)\n",
      "jinja2.exceptions.TemplateNotFound: index.html\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Jan/2025 20:16:37] \"\u001b[35m\u001b[1mGET /?authuser=0 HTTP/1.1\u001b[0m\" 500 -\n"
     ]
    }
   ],
   "source": [
    "# compiled all code into one box\n",
    "\n",
    "# Requires index.html template to be placed into templates/index.html to work\n",
    "\n",
    "\n",
    "# Url: To input textbook url. Recognized as a .pdf format. Example url: https://www.mrbigler.com/downloads/Notes-Physics-1.pdf\n",
    "# Prompt: To input question. Recognized as a string format. Example prompt: What is momentum?\n",
    "\n",
    "# Response for now is given as a JSON response with question being your prompt and response being\n",
    "# the tutor's answer.\n",
    "\n",
    "# TO USE, PRESS THE LINK DOWN IN THE OUTPUT.\n",
    "# please press CANCEL to the prompt it asks about restarting runtime to use new packages\n",
    "# Final code with everything attached.\n",
    "#Question Answering system using a pdf\n",
    "#Install the libraries\n",
    "!pip install -q langchain\n",
    "!pip install -q langchain_community\n",
    "!pip install -q langchain_openai\n",
    "!pip install -q openai\n",
    "!pip install -q langchain_core\n",
    "!pip install -q pypdf\n",
    "!pip install -q chromadb\n",
    "!pip install -q Flask\n",
    "!pip install -q markdown\n",
    "!pip install -q langchain guardrails-ai\n",
    "!pip install -q guardrails-ai\n",
    "\n",
    "# configure with api key\n",
    "!guardrails configure\n",
    "\n",
    "# install guardrails libraries\n",
    "!guardrails hub install hub://guardrails/ban_list\n",
    "!guardrails hub install hub://guardrails/bias_check\n",
    "!guardrails hub install hub://guardrails/nsfw_text\n",
    "!guardrails hub install hub://guardrails/profanity_free\n",
    "!guardrails hub install hub://guardrails/logic_check\n",
    "!guardrails hub install hub://cartesia/mentions_drugs\n",
    "!guardrails hub install hub://guardrails/politeness_check\n",
    "!guardrails hub install hub://guardrails/toxic_language\n",
    "\n",
    "##Imports for Baseline QA Pipeline\n",
    "from langchain.document_loaders import PyPDFLoader # for loading the pdf\n",
    "from langchain_openai import OpenAIEmbeddings # for creating embeddings\n",
    "from langchain.vectorstores import Chroma # for the vectorization part\n",
    "from langchain.chains import RetrievalQA #For the retrieval QA chain part # apparently deprecated\n",
    "from langchain_openai import ChatOpenAI #for getting an LLM for QA chain\n",
    "#from langchain_core.output_parsers import StrOutputParser #Not used currently, leaving, as can be used for parsing output from LLM\n",
    "#from langchain_core.runnables import RunnablePassthrough #Not used currently, leaving, as can be used for getting LLM output\n",
    "from langchain.prompts import ChatPromptTemplate #for setting up prompts\n",
    "\n",
    "\n",
    "#Guardrails stuff\n",
    "from guardrails import Guard, OnFailAction\n",
    "from guardrails.hub import BanList, BiasCheck, NSFWText, ProfanityFree, LogicCheck, MentionsDrugs, PolitenessCheck#, ToxicLanguage# Updated import\n",
    "#import guardrails.hub #which needs fuzzysearch, which is already downloaded\n",
    "#from guardrails.datatypes import String\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "#Setup openai key\n",
    "import os\n",
    "import openai\n",
    "from getpass import getpass\n",
    "print(\"Please enter Open AI KEY\")\n",
    "OPENAI_API_KEY = getpass()\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "from guardrails import Guard\n",
    "\n",
    "\n",
    "guard = Guard().use_many(\n",
    "    BiasCheck(\n",
    "        threshold=0.5,\n",
    "        on_fail=\"exception\"\n",
    "    ),\n",
    "\n",
    "    NSFWText(\n",
    "        threshold=0.8,\n",
    "        validation_method=\"sentence\"\n",
    "    ),\n",
    "\n",
    "    ProfanityFree(\n",
    "        on_fail = \"exception\"\n",
    "    ),\n",
    "\n",
    "    LogicCheck(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        on_fail=\"exception\"\n",
    "    ),\n",
    "\n",
    "    MentionsDrugs(\n",
    "        on_fail = \"exception\"\n",
    "    ),\n",
    "\n",
    "    PolitenessCheck(\n",
    "        llm_callable=\"gpt-3.5-turbo\",\n",
    "        on_fail = \"exception\"\n",
    "    ),\n",
    "\n",
    "    ToxicLanguage(\n",
    "        threshold=0.5,\n",
    "        validation_method=\"sentence\",\n",
    "        on_fail=\"exception\"\n",
    "    )\n",
    "\n",
    ")\n",
    "\n",
    "try:\n",
    "  guard.validate(\"I hate you!\")\n",
    "  print(\"hello\")\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "#Setup GenerateStudyPlan pipeline\n",
    "class GenerateStudyPlan:\n",
    "    def __init__(self):\n",
    "        self.doc = \"tutor_textbook.pdf\"\n",
    "        self.loader = PyPDFLoader(self.doc)\n",
    "\n",
    "        # Load the document and store it in the 'data' variable\n",
    "        self.data = self.loader.load_and_split()\n",
    "\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.vectordb = Chroma.from_documents(self.data, embedding=self.embeddings,\n",
    "                                 persist_directory=\".\")\n",
    "\n",
    "        # Initialize a language model with ChatOpenAI\n",
    "        self.llm = ChatOpenAI(model_name= 'gpt-3.5-turbo', temperature=0.6)\n",
    "\n",
    "        #Setup a prompt template\n",
    "        template = \"\"\"\\\n",
    "            You are an assistant for generating study plans on a singular subject.\n",
    "\n",
    "        Use the following pieces of retrieved context to answer the question.\n",
    "\n",
    "        If the user has given a topic to study or topics that they need focus on,\n",
    "        make the plan more focused on those topics.\n",
    "\n",
    "        Give a nice and detailed study plan. If the user doesnt specify a type of \n",
    "        study plan (schedule wise), you can come up with your own, hourly or daily \n",
    "        plan, or you can ask the user to give the same instructions but with the \n",
    "        study plan of their choice. \n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Answer:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "        chain_type_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "\n",
    "        # 1. Vectorstore-based retriever\n",
    "        self.vectorstore_retriever = self.vectordb.as_retriever()\n",
    "\n",
    "        # Initialize a RetrievalQA chain with the language model and vector database retriever\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(self.llm, retriever= self.vectorstore_retriever, chain_type_kwargs=chain_type_kwargs)\n",
    "        self.chat_history = []  # Initialize chat history\n",
    "\n",
    "    def update_chat_history(self, question, answer):\n",
    "        self.chat_history.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "    def build_combined_context(self):\n",
    "        \"\"\"Combine chat history and document context.\"\"\"\n",
    "        # Combine all previous chat history\n",
    "        chat_context = \"\\n\".join([f\"Q: {entry['question']}\\nA: {entry['answer']}\" for entry in self.chat_history])\n",
    "        \n",
    "        # Fetch relevant context from the vector store based on the current question\n",
    "        if self.chat_history:\n",
    "            current_question = self.chat_history[-1]['question']\n",
    "            context_from_db = self.vectorstore_retriever.get_relevant_documents(current_question)\n",
    "        else:\n",
    "            context_from_db = self.vectorstore_retriever.get_relevant_documents(\"\")\n",
    "\n",
    "        # Convert the list of context documents into a string\n",
    "        context_str = \"\\n\".join([doc.page_content for doc in context_from_db])\n",
    "\n",
    "        # Combine both chat history and the document context\n",
    "        combined_context = f\"Chat history:\\n{chat_context}\\n\\nContext from the document:\\n{context_str}\"\n",
    "        \n",
    "        return combined_context\n",
    "\n",
    "\n",
    "    def invoke(self, input_dict):\n",
    "        question = input_dict.get(\"question\")\n",
    "        #here put a if-else that returns true or false depending on if question passes\n",
    "        # guardrails checks\n",
    "        if (self.guardrails(question) == False):\n",
    "          print(\"It has failed (this is only a message to debug)\\n\")\n",
    "          return \"Sorry, please ask another question\"\n",
    "        \n",
    "        combined_context = self.build_combined_context()\n",
    "\n",
    "        result = self.qa_chain.invoke({\n",
    "            \"query\": question,\n",
    "            \"context\": combined_context\n",
    "        })\n",
    "\n",
    "        self.update_chat_history(question, result['result'])\n",
    "        if (self.guardrails(question) == False):\n",
    "          print(\"It has failed (this is only a message to debug)\\n\")\n",
    "          return \"Sorry, please ask another question\"\n",
    "        else:\n",
    "          return result\n",
    "\n",
    "    def guardrails(self, input):\n",
    "        #if guardrails return true send back whatever the input is,\n",
    "        #else send back an error message\n",
    "        try:\n",
    "            guard.validate(input)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            return False\n",
    "\n",
    "from flask import Flask, render_template, request, redirect, url_for\n",
    "import markdown\n",
    "\n",
    "filepath = \"./tutor_textbook.pdf\"\n",
    "ALLOWED_EXTENSIONS = {'txt', 'pdf', 'docx', 'png', 'jpg', 'jpeg', 'gif'}\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\", methods=[\"GET\", \"POST\"])\n",
    "def index():\n",
    "    global url_data, prompt_data  # Access global variables\n",
    "\n",
    "    if request.method == \"POST\":\n",
    "        url_data = request.form.get(\"url\")\n",
    "        print(\"URL: \", url_data)\n",
    "        if 'file' not in request.files:\n",
    "            print('No file uploaded!')\n",
    "        else:\n",
    "          file = request.files['file']\n",
    "          file.save(filepath)\n",
    "          print(\"File saved:\", filepath)\n",
    "        if (url_data != \"\"):\n",
    "            !curl {url_data} > tutor_textbook.pdf\n",
    "        print(\"File: \",file)\n",
    "        prompt_data = request.form.get(\"prompt\")\n",
    "        base_qa_pipeline = BaseQAPipeline()\n",
    "        result = base_qa_pipeline.invoke({'question' : prompt_data})\n",
    "        print(result)\n",
    "        return render_template(\"index.html\", result=result)\n",
    "\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route('/how-it-works', methods=['GET'])\n",
    "def how_it_works():\n",
    "    return render_template('how-it-works.html')\n",
    "\n",
    "@app.route('/generate-plan', methods=['GET', \"POST\"])\n",
    "def generate_plan():\n",
    "    if request.method == \"POST\":\n",
    "        if 'file' not in request.files:\n",
    "            print('No file uploaded!')\n",
    "        else:\n",
    "          file = request.files['file']\n",
    "          file.save(filepath)\n",
    "          print(\"File saved:\", filepath)\n",
    "        print(\"File: \",file)\n",
    "        prompt_data = request.form.get(\"prompt\")\n",
    "        generate_plan = GenerateStudyPlan()\n",
    "        result = generate_plan.invoke({'question' : prompt_data})\n",
    "        result['result'] = markdown.markdown(result['result'])\n",
    "        print(result)\n",
    "        return render_template(\"generate-plan.html\", result=result)\n",
    "\n",
    "    return render_template(\"generate-plan.html\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    app.run(port=5001)\n",
    "# Requires index.html template to be placed into templates/index.html to work\n",
    "\n",
    "\n",
    "# Url: To input textbook url. Recognized as a .pdf format. Example url: https://www.mrbigler.com/downloads/Notes-Physics-1.pdf\n",
    "# Prompt: To input question. Recognized as a string format. Example prompt: What is momentum?\n",
    "\n",
    "# Response for now is given as a JSON response with question being your prompt and response being\n",
    "# the tutor's answer.\n",
    "\n",
    "# TO USE, PRESS THE LINK DOWN IN THE OUTPUT.\n",
    "# please press CANCEL to the prompt it asks about restarting runtime to use new packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_scuFpyxM-I"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
